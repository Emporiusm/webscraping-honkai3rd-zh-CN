{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "import openpyxl\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Honkai:\n",
    "    '''class Honkai'''\n",
    "\n",
    "    def __init__(self, source, from_page=None, to_page=None):\n",
    "        self.source = source\n",
    "        self.from_page = from_page\n",
    "        self.to_page = to_page\n",
    "        if source.endswith('pkl'):\n",
    "            with open(pickle, 'rb') as pkl:\n",
    "                souplist = pkl.read()\n",
    "                return souplist\n",
    "        elif source == 'web' and type(from_page) == int and type(to_page) == int:\n",
    "            pages           = range(from_page,to_page+1)\n",
    "            url_part        = 'https://3rdguide.com/web/valk/detail?id='\n",
    "            urls            = [url_part + str(page) for page in pages]\n",
    "            path            = os.path.abspath(os.getcwd())\n",
    "            progress        = from_page\n",
    "            for url in urls:\n",
    "                html = urlopen(url).read()\n",
    "                soup = BeautifulSoup(html.decode('utf-8'),'html.parser')\n",
    "                souplist.append(soup)\n",
    "                print(str(progress) + ' of ' + str(to_page) + ' pages loading...')\n",
    "                time.sleep(np.random.randint(1,3))\n",
    "                progress = progress + 1\n",
    "            return souplist\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def get_skills(self):\n",
    "        for soup in souplist:\n",
    "    dic = {}\n",
    "    dic.update(enumerate(soup.stripped_strings))\n",
    "    if dic.get(6) == None:\n",
    "        pass\n",
    "    else:\n",
    "        name = dic.get(6)\n",
    "        categories = [category.get_text() for category in soup.findAll('p',class_='item1')]\n",
    "        major_name = [category.get_text() for category in soup.findAll('p',class_='item2')]\n",
    "        string1 = str()\n",
    "        major_detail = [string1 + category.get_text() for category in soup.findAll('p',class_='msg')]\n",
    "        minor_name = [category.get_text() for category in soup.findAll('p',class_='item1_1')]\n",
    "        string2 = str()\n",
    "        minor_detail = [string2 + category.get_text() for category in soup.findAll('div',class_='item3')]\n",
    "        major = pd.DataFrame([major_name,major_detail],index=['技能','描述'])\n",
    "        minor = pd.DataFrame([minor_name[1:-1],minor_detail],index=['技能','描述'])\n",
    "        concat = pd.concat([major,minor],axis=1).transpose()\n",
    "        concat['女武神'] = name\n",
    "        concat['Page'] = page\n",
    "        concat['描述'] = concat['描述'].astype(str).str.strip()\n",
    "        page = page + 1\n",
    "        df = df.append(concat)\n",
    "    df = df.set_index(df['女武神'],drop=True)\n",
    "    df = df.dropna()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595343523790",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}